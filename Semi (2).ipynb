{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0316b4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:46:04.743882: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-23 17:46:04.747840: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 17:46:04.799666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 17:46:05.639799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "CUDA version: 11.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from randaugment import RandAugmentMC\n",
    "\n",
    "\n",
    "\n",
    "CUDA = True\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "seed = 42\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "if CUDA:\n",
    "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11091f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:46:09.557538: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-23 17:46:09.561272: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset, info = tfds.load('colorectal_histology', with_info=True, as_supervised=True)\n",
    "dataset = dataset['train'].batch(len(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3e8957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:46:14.541408: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Transform the dataset into pytorch\n",
    "for images, labels in dataset:\n",
    "    images_tensor = torch.tensor(images.numpy(), dtype=torch.float)\n",
    "    images_tensor = images_tensor.permute(0, 3, 1, 2)\n",
    "    labels_tensor = torch.tensor(labels.numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb41d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorectalHistDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].clone().detach()\n",
    "        label = self.labels[idx].clone().detach()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a555e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorectalHistDatasetSemi(Dataset):\n",
    "    def __init__(self, images, labels, transform_w=None, transform_s=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform_w = transform_w\n",
    "        self.transform_s = transform_s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].clone().detach()\n",
    "        label = self.labels[idx].clone().detach()\n",
    "\n",
    "        if self.transform_w:\n",
    "            image_w = self.transform_w(image)\n",
    "        else:\n",
    "            image_w = image\n",
    "\n",
    "        if self.transform_s:\n",
    "            image_s = self.transform_s(image)\n",
    "        else:\n",
    "            image_s = image\n",
    "\n",
    "\n",
    "        return image_w, image_s, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d7794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataset, test_size=0.2):\n",
    "\n",
    "    labels = np.array([ins[-1] for ins in dataset])\n",
    "\n",
    "    # Indices for each class\n",
    "    class_indices = [np.where(labels == class_label)[0] for class_label in np.unique(labels)]\n",
    "\n",
    "    # Split each class's indices into train and test\n",
    "    train_indices, test_indices = [], []\n",
    "    for indices in class_indices:\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(test_size * len(indices)))\n",
    "        train_indices.extend(indices[split:])\n",
    "        test_indices.extend(indices[:split])\n",
    "\n",
    "    # Create subset for train and test\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "    return train_subset, test_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c3ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for supervised learning\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image to apply transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip with 50% probability\n",
    "    # Add any additional transformations here (e.g., resizing, normalization)\n",
    "    transforms.RandomResizedCrop(size=(150, 150), antialias=True),\n",
    "    transforms.ToTensor(),  # Convert PIL Image back to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = ColorectalHistDataset(images_tensor, labels_tensor, transform)\n",
    "train_subset, test_subset = stratified_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1446aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for semi-supervised learning\n",
    "transform_w = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image to apply transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip with 50% probability\n",
    "    transforms.ToTensor(),  # Convert PIL Image back to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_s = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image to apply transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip with 50% probability\n",
    "    RandAugmentMC(n=2, m=10),\n",
    "    transforms.ToTensor(),  # Convert PIL Image back to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aee8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base learner\n",
    "\n",
    "class CRCClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(CRCClassifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "        self.conv_layers = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "model = CRCClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2872641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe32ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ffe6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "eval_step = 200\n",
    "threshold = 0.95\n",
    "batch_size = 40\n",
    "ratio = 5\n",
    "lambda_u = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c438ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled instances selection\n",
    "dataset = ColorectalHistDatasetSemi(images_tensor, labels_tensor, transform_w, transform_s)\n",
    "train_subset, test_subset = stratified_split(dataset)\n",
    "unlabeled_subset, labeled_subset = stratified_split(train_subset, test_size=0.02)\n",
    "labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(test_subset, batch_size=40, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f349f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave(x, batch_size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, batch_size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "def train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u):\n",
    "    model.train()\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_s = 0.0\n",
    "        loss_u = 0.0\n",
    "        for batch_idx in range(eval_step):\n",
    "            try:\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = iter(labeled_loader)\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "\n",
    "            try:\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "\n",
    "            inputs_x = inputs_x.to(device)\n",
    "            targets_x = targets_x.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), ratio*2+1).to(device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, ratio*2+1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            # Compute the loss for labeled data\n",
    "            Lx = criterion(logits_x, targets_x)\n",
    "            loss_s += Lx.item() * inputs_x.size(0)\n",
    "\n",
    "            # Compute the pseudo-labels and loss for unlabeled data\n",
    "            probs = torch.softmax(logits_u_w, dim=-1)\n",
    "            max_probs, targets_u = torch.max(probs, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "\n",
    "            loss_u += Lu.item() * mask.sum()\n",
    "\n",
    "            # Total loss\n",
    "            loss = Lx + lambda_u * Lu\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        loss_s = loss_s / (eval_step * batch_size)\n",
    "        loss_u = loss_u / (eval_step * batch_size * ratio)\n",
    "\n",
    "        print(f'Supervised Loss: {loss_s}, Unsupervised Loss: {loss_u}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs_w, inputs_s, labels in val_loader:\n",
    "                inputs = inputs_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_subset)\n",
    "        val_acc = val_corrects.double() / len(test_subset)\n",
    "        print(f'Validation Loss: {val_loss}, Acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eb72776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1): \n",
    "    model.train()\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_s = 0.0\n",
    "        loss_u = 0.0\n",
    "        for batch_idx in range(eval_step):\n",
    "            try:\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = iter(labeled_loader)\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "\n",
    "            try:\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "                if len(inputs_u_w) < batch_size*ratio:\n",
    "                    unlabeled_iter = iter(unlabeled_loader)\n",
    "                    inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "\n",
    "            inputs_x = inputs_x.to(device)\n",
    "            targets_x = targets_x.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "            # print(inputs_x.shape[0], inputs_u_w.shape[0], inputs_u_s.shape[0])\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), ratio*2+1).to(device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, ratio*2+1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            # Compute the loss for labeled data\n",
    "            Lx = criterion(logits_x, targets_x)\n",
    "            loss_s += Lx.item() * inputs_x.size(0)\n",
    "\n",
    "            # Compute the pseudo-labels and loss for unlabeled data\n",
    "            probs = torch.softmax(logits_u_w, dim=-1)\n",
    "            max_probs, targets_u = torch.max(probs, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "\n",
    "            loss_u += Lu.item() * mask.sum()\n",
    "\n",
    "            # Total loss\n",
    "            loss = Lx + lambda_u * Lu\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        loss_s = loss_s / (eval_step * batch_size)\n",
    "        loss_u = loss_u / (eval_step * batch_size * ratio)\n",
    "\n",
    "        print(f'Supervised Loss: {loss_s}, Unsupervised Loss: {loss_u}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs_w, inputs_s, labels in val_loader:\n",
    "                inputs = inputs_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_subset)\n",
    "        val_acc = val_corrects.double() / len(test_subset)\n",
    "        print(f'Validation Loss: {val_loss}, Acc: {val_acc}')\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "        \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd9b167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixmatch_active(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1): \n",
    "    model.train()\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_s = 0.0\n",
    "        loss_u = 0.0\n",
    "        for batch_idx in range(eval_step):\n",
    "            try:\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = iter(labeled_loader)\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "\n",
    "            try:\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "                if len(inputs_u_w) < batch_size*ratio:\n",
    "                    unlabeled_iter = iter(unlabeled_loader)\n",
    "                    inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "\n",
    "            inputs_x = inputs_x.to(device)\n",
    "            targets_x = targets_x.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "            # print(inputs_x.shape[0], inputs_u_w.shape[0], inputs_u_s.shape[0])\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), ratio*2+1).to(device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, ratio*2+1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            # Compute the loss for labeled data\n",
    "            Lx = criterion(logits_x, targets_x)\n",
    "            loss_s += Lx.item() * inputs_x.size(0)\n",
    "\n",
    "            # Compute the pseudo-labels and loss for unlabeled data\n",
    "            probs = torch.softmax(logits_u_w, dim=-1)\n",
    "            max_probs, targets_u = torch.max(probs, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "\n",
    "            loss_u += Lu.item() * mask.sum()\n",
    "\n",
    "            # Total loss\n",
    "            loss = Lx + lambda_u * Lu\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        loss_s = loss_s / (eval_step * batch_size)\n",
    "        loss_u = loss_u / (eval_step * batch_size * ratio)\n",
    "\n",
    "        # print(f'Supervised Loss: {loss_s}, Unsupervised Loss: {loss_u}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs_w, inputs_s, labels in val_loader:\n",
    "                inputs = inputs_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_subset)\n",
    "        val_acc = val_corrects.double() / len(test_subset)\n",
    "        # print(f'Validation Loss: {val_loss}, Acc: {val_acc}')\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "344ce5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling(model, unlabeled_subset, strategy = 'uncertainty', forward_passes = 100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        if strategy == 'uncertainty':\n",
    "            for _, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "            safe_probabilities = all_predictions.clamp(min=1e-9)\n",
    "            scores = -torch.sum(safe_probabilities * torch.log(safe_probabilities), dim=1)\n",
    "            _, indices = torch.topk(scores, batch_size)\n",
    "        elif strategy == 'diversity':\n",
    "            for inputs_w, _, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "            max_probs, targets_u = torch.max(all_predictions, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return indices\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a779bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling(model, unlabeled_subset, strategy = 'uncertainty', forward_passes = 100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        if strategy == 'uncertainty':\n",
    "            all_predictions = []\n",
    "            for _, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "            safe_probabilities = all_predictions.clamp(min=1e-9)\n",
    "            scores = -torch.sum(safe_probabilities * torch.log(safe_probabilities), dim=1)\n",
    "            _, indices = torch.topk(scores, batch_size)\n",
    "        elif strategy == 'diversity':\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            for inputs_w, _, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            similarity_matrix = torch.zeros((len(all_predictions), len(all_predictions)))\n",
    "            for i in range(len(all_predictions)):\n",
    "                for j in range(i+1, len(all_predictions)):\n",
    "                    similarity_matrix[i, j] = torch.cosine_similarity(all_predictions[i], all_predictions[j])\n",
    "                    similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "            \n",
    "            # Select the indices with the highest diversity\n",
    "            selected_indices = []\n",
    "            while len(selected_indices) < batch_size:\n",
    "                # Find the index with the lowest average similarity to the already selected indices\n",
    "                min_similarity = float('inf')\n",
    "                min_index = None\n",
    "                for i in range(len(all_predictions)):\n",
    "                    if i not in selected_indices:\n",
    "                        similarity = torch.mean(similarity_matrix[i, selected_indices])\n",
    "                        if similarity < min_similarity:\n",
    "                            min_similarity = similarity\n",
    "                            min_index = i\n",
    "                \n",
    "                # Add the selected index to the list and update the similarity matrix\n",
    "                selected_indices.append(min_index)\n",
    "                similarity_matrix = torch.cat((similarity_matrix[:min_index, :], similarity_matrix[min_index+1:, :]), dim=0)\n",
    "                similarity_matrix = torch.cat((similarity_matrix[:, :min_index], similarity_matrix[:, min_index+1:]), dim=1)\n",
    "        elif strategy == 'consistency':\n",
    "            all_predictions_s = []\n",
    "            all_predictions_w = []\n",
    "            for inputs_w, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions_w = []\n",
    "                mean_predictions_s = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs_s = inputs_s.to(device)\n",
    "                    inputs_w = inputs_w.to(device)\n",
    "                    outputs_s = model(inputs_s)\n",
    "                    outputs_w = model(inputs_w)\n",
    "                    probs_s = torch.softmax(outputs_s, dim=-1)\n",
    "                    probs_w = torch.softmax(outputs_w, dim=-1)\n",
    "                    mean_predictions_s.append(probs_s.unsqueeze(0))\n",
    "                    mean_predictions_w.append(probs_w.unsqueeze(0))\n",
    "                mean_predictions_s = torch.cat(mean_predictions_s)\n",
    "                mean_predictions_s = torch.mean(mean_predictions_s, 0)\n",
    "                mean_predictions_w = torch.cat(mean_predictions_w)\n",
    "                mean_predictions_w = torch.mean(mean_predictions_w, 0)\n",
    "                \n",
    "                all_predictions_s.append(mean_predictions_s)\n",
    "                all_predictions_w.append(mean_predictions_w)\n",
    "            all_predictions_s = torch.cat(all_predictions_s)\n",
    "            all_predictions_w = torch.cat(all_predictions_w)\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            scores = -torch.cosine_similarity(all_predictions_s, all_predictions_w, dim = -1)\n",
    "            _, indices = torch.topk(scores, batch_size)\n",
    "        elif strategy == 'combined':\n",
    "            all_predictions_s = []\n",
    "            all_predictions_w = []\n",
    "            for inputs_w, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions_w = []\n",
    "                mean_predictions_s = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs_s = inputs_s.to(device)\n",
    "                    inputs_w = inputs_w.to(device)\n",
    "                    outputs_s = model(inputs_s)\n",
    "                    outputs_w = model(inputs_w)\n",
    "                    probs_s = torch.softmax(outputs_s, dim=-1)\n",
    "                    probs_w = torch.softmax(outputs_w, dim=-1)\n",
    "                    mean_predictions_s.append(probs_s.unsqueeze(0))\n",
    "                    mean_predictions_w.append(probs_w.unsqueeze(0))\n",
    "                mean_predictions_s = torch.cat(mean_predictions_s)\n",
    "                mean_predictions_s = torch.mean(mean_predictions_s, 0)\n",
    "                mean_predictions_w = torch.cat(mean_predictions_w)\n",
    "                mean_predictions_w = torch.mean(mean_predictions_w, 0)\n",
    "                \n",
    "                all_predictions_s.append(mean_predictions_s)\n",
    "                all_predictions_w.append(mean_predictions_w)\n",
    "            all_predictions_s = torch.cat(all_predictions_s)\n",
    "            all_predictions_w = torch.cat(all_predictions_w)\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            consistency_scores = torch.cosine_similarity(all_predictions_s, all_predictions_w, dim = -1)\n",
    "            safe_probabilities = all_predictions_s.clamp(min=1e-9)\n",
    "            uncertainty_scores = -torch.sum(safe_probabilities * torch.log(safe_probabilities), dim = 1)\n",
    "            combined_scores = consistency_scores * uncertainty_scores\n",
    "            _, indices = torch.topk(combined_scores, batch_size)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return indices\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e77333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5, 10):\n",
    "    set_seed(seed)\n",
    "    model = CRCClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    curr_train_data_portion = 0.01*i\n",
    "    print(f'Currently {curr_train_data_portion*100}% of the train data is being used')\n",
    "    if i == 5:\n",
    "        unlabeled_subset, labeled_subset = stratified_split(train_subset, test_size=curr_train_data_portion)\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "        acc = train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)\n",
    "        #torch.save(model.state_dict(), 'semi_5_percents_combined.pth')\n",
    "        indices = active_sampling(model, unlabeled_subset, 'combined')\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    else:\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "        acc = train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)\n",
    "        indices = active_sampling(model, unlabeled_subset, 'combined')\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    print(f'best_acc:{acc}')\n",
    "    with open('semi_val_accs_act_combined_0.05_3.txt','a+') as f:\n",
    "        f.write(f'{acc}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
