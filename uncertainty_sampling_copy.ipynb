{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cc0578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 17:09:04.462082: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-02 17:09:04.522908: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-02 17:09:04.615020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 17:09:06.688039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "CUDA version: 11.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "\n",
    "\n",
    "\n",
    "CUDA = True\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "seed = 42\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "if CUDA:\n",
    "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79554fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec273ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d53ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 17:09:11.379306: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-02 17:09:11.381985: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset, info = tfds.load('colorectal_histology', with_info=True, as_supervised=True)\n",
    "dataset = dataset['train'].batch(len(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8cb9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 17:09:16.248084: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Transform the dataset into pytorch\n",
    "for images, labels in dataset:\n",
    "    images_tensor = torch.tensor(images.numpy(), dtype=torch.float)\n",
    "    images_tensor = images_tensor.permute(0, 3, 1, 2)\n",
    "    labels_tensor = torch.tensor(labels.numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4488bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorectalHistDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].clone().detach()\n",
    "        label = self.labels[idx].clone().detach()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738b7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataset, test_size=0.2):\n",
    "\n",
    "    labels = np.array([label for _, label in dataset])\n",
    "\n",
    "    # Indices for each class\n",
    "    class_indices = [np.where(labels == class_label)[0] for class_label in np.unique(labels)]\n",
    "\n",
    "    # Split each class's indices into train and test\n",
    "    train_indices, test_indices = [], []\n",
    "    for indices in class_indices:\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(test_size * len(indices)))\n",
    "        train_indices.extend(indices[split:])\n",
    "        test_indices.extend(indices[:split])\n",
    "\n",
    "    # Create subset for train and test\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "    return train_subset, test_subset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image to apply transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip with 50% probability\n",
    "    transforms.ToTensor(),  # Convert PIL Image back to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb2f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ColorectalHistDataset(images_tensor, labels_tensor, transform)\n",
    "train_subset, test_subset = stratified_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b240e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base learner\n",
    "\n",
    "class CRCClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(CRCClassifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        resnet18.fc =  nn.Sequential(nn.Dropout(0.25), nn.Linear(resnet18.fc.in_features, num_classes))\n",
    "        self.conv_layers = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b76296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(dataset, test_size=0.8):\n",
    "    num_samples = len(dataset)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(test_size * num_samples))\n",
    "    train_indices, test_indices = indices[split:], indices[:split]\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "    return train_subset, test_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7090d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRCClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "batch_size = 40\n",
    "val_loader = DataLoader(test_subset, batch_size=40, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53a22ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multistep(model, train_loader, optimizer, num_epochs):\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_subset)\n",
    "        val_acc = val_corrects.double() / len(test_subset)\n",
    "        # print(f\"val_acc:{val_acc}\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "        \n",
    "    return best_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d11a8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling(model, unlabeled_subset, strategy = 'uncertainty', forward_passes = 10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        if strategy == 'uncertainty':\n",
    "            for inputs, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "            safe_probabilities = all_predictions.clamp(min=1e-9)\n",
    "            scores = -torch.sum(safe_probabilities * torch.log(safe_probabilities), dim=1)\n",
    "            _, indices = torch.topk(scores, batch_size)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return indices\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fea919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 10.0% of the train data is being used\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 51):\n",
    "    set_seed(seed)\n",
    "    model = CRCClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    curr_train_data_portion = 0.01*i\n",
    "    print(f'Currently {curr_train_data_portion*100}% of the train data is being used')\n",
    "    if i == 10:\n",
    "        unlabeled_subset, labeled_subset = stratified_split(train_subset, test_size=curr_train_data_portion)\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        acc = train_multistep(model, labeled_loader, optimizer, num_epochs)\n",
    "        #torch.save(model.state_dict(), 'semi_5_percents_combined.pth')\n",
    "        indices = active_sampling(model, unlabeled_subset, 'uncertainty')\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    else:\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        acc = train_multistep(model, labeled_loader, optimizer, num_epochs)\n",
    "        indices = active_sampling(model, unlabeled_subset, 'uncertainty')\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    print(f'best_acc:{acc}')\n",
    "    with open('supervised_val_accs_uncertainty_10_best.txt','a+') as f:\n",
    "        f.write(f'{acc}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602e0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
