{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0316b4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:46:04.743882: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-23 17:46:04.747840: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 17:46:04.799666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 17:46:05.639799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "CUDA version: 11.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from randaugment import RandAugmentMC\n",
    "\n",
    "\n",
    "\n",
    "CUDA = True\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "seed = 42\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "if CUDA:\n",
    "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11091f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:46:09.557538: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-23 17:46:09.561272: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset, info = tfds.load('colorectal_histology', with_info=True, as_supervised=True)\n",
    "dataset = dataset['train'].batch(len(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3e8957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:46:14.541408: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Transform the dataset into pytorch\n",
    "for images, labels in dataset:\n",
    "    images_tensor = torch.tensor(images.numpy(), dtype=torch.float)\n",
    "    images_tensor = images_tensor.permute(0, 3, 1, 2)\n",
    "    labels_tensor = torch.tensor(labels.numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb41d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorectalHistDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].clone().detach()\n",
    "        label = self.labels[idx].clone().detach()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a555e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorectalHistDatasetSemi(Dataset):\n",
    "    def __init__(self, images, labels, transform_w=None, transform_s=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform_w = transform_w\n",
    "        self.transform_s = transform_s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].clone().detach()\n",
    "        label = self.labels[idx].clone().detach()\n",
    "\n",
    "        if self.transform_w:\n",
    "            image_w = self.transform_w(image)\n",
    "        else:\n",
    "            image_w = image\n",
    "\n",
    "        if self.transform_s:\n",
    "            image_s = self.transform_s(image)\n",
    "        else:\n",
    "            image_s = image\n",
    "\n",
    "\n",
    "        return image_w, image_s, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d7794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataset, test_size=0.2):\n",
    "\n",
    "    labels = np.array([ins[-1] for ins in dataset])\n",
    "\n",
    "    # Indices for each class\n",
    "    class_indices = [np.where(labels == class_label)[0] for class_label in np.unique(labels)]\n",
    "\n",
    "    # Split each class's indices into train and test\n",
    "    train_indices, test_indices = [], []\n",
    "    for indices in class_indices:\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(test_size * len(indices)))\n",
    "        train_indices.extend(indices[split:])\n",
    "        test_indices.extend(indices[:split])\n",
    "\n",
    "    # Create subset for train and test\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "    return train_subset, test_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c3ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for supervised learning\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image to apply transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip with 50% probability\n",
    "    # Add any additional transformations here (e.g., resizing, normalization)\n",
    "    transforms.RandomResizedCrop(size=(150, 150), antialias=True),\n",
    "    transforms.ToTensor(),  # Convert PIL Image back to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = ColorectalHistDataset(images_tensor, labels_tensor, transform)\n",
    "train_subset, test_subset = stratified_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1446aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for semi-supervised learning\n",
    "transform_w = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image to apply transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip with 50% probability\n",
    "    transforms.ToTensor(),  # Convert PIL Image back to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_s = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image to apply transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip with 50% probability\n",
    "    RandAugmentMC(n=2, m=10),\n",
    "    transforms.ToTensor(),  # Convert PIL Image back to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aee8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base learner\n",
    "\n",
    "class CRCClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(CRCClassifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "        self.conv_layers = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "model = CRCClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f5bfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1947, -0.8845,  0.7924,  1.2290,  0.5628, -0.1965,  0.4304, -0.5121,\n",
       "        -2.1681,  1.2008], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2872641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe32ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ffe6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "eval_step = 200\n",
    "threshold = 0.95\n",
    "batch_size = 40\n",
    "ratio = 5\n",
    "lambda_u = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c438ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled instances selection\n",
    "dataset = ColorectalHistDatasetSemi(images_tensor, labels_tensor, transform_w, transform_s)\n",
    "train_subset, test_subset = stratified_split(dataset)\n",
    "unlabeled_subset, labeled_subset = stratified_split(train_subset, test_size=0.02)\n",
    "labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(test_subset, batch_size=40, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f349f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave(x, batch_size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, batch_size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "def train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u):\n",
    "    model.train()\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_s = 0.0\n",
    "        loss_u = 0.0\n",
    "        for batch_idx in range(eval_step):\n",
    "            try:\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = iter(labeled_loader)\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "\n",
    "            try:\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "\n",
    "            inputs_x = inputs_x.to(device)\n",
    "            targets_x = targets_x.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), ratio*2+1).to(device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, ratio*2+1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            # Compute the loss for labeled data\n",
    "            Lx = criterion(logits_x, targets_x)\n",
    "            loss_s += Lx.item() * inputs_x.size(0)\n",
    "\n",
    "            # Compute the pseudo-labels and loss for unlabeled data\n",
    "            probs = torch.softmax(logits_u_w, dim=-1)\n",
    "            max_probs, targets_u = torch.max(probs, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "\n",
    "            loss_u += Lu.item() * mask.sum()\n",
    "\n",
    "            # Total loss\n",
    "            loss = Lx + lambda_u * Lu\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        loss_s = loss_s / (eval_step * batch_size)\n",
    "        loss_u = loss_u / (eval_step * batch_size * ratio)\n",
    "\n",
    "        print(f'Supervised Loss: {loss_s}, Unsupervised Loss: {loss_u}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs_w, inputs_s, labels in val_loader:\n",
    "                inputs = inputs_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_subset)\n",
    "        val_acc = val_corrects.double() / len(test_subset)\n",
    "        print(f'Validation Loss: {val_loss}, Acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eb72776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1): \n",
    "    model.train()\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_s = 0.0\n",
    "        loss_u = 0.0\n",
    "        for batch_idx in range(eval_step):\n",
    "            try:\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = iter(labeled_loader)\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "\n",
    "            try:\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "                if len(inputs_u_w) < batch_size*ratio:\n",
    "                    unlabeled_iter = iter(unlabeled_loader)\n",
    "                    inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "\n",
    "            inputs_x = inputs_x.to(device)\n",
    "            targets_x = targets_x.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "            # print(inputs_x.shape[0], inputs_u_w.shape[0], inputs_u_s.shape[0])\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), ratio*2+1).to(device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, ratio*2+1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            # Compute the loss for labeled data\n",
    "            Lx = criterion(logits_x, targets_x)\n",
    "            loss_s += Lx.item() * inputs_x.size(0)\n",
    "\n",
    "            # Compute the pseudo-labels and loss for unlabeled data\n",
    "            probs = torch.softmax(logits_u_w, dim=-1)\n",
    "            max_probs, targets_u = torch.max(probs, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "\n",
    "            loss_u += Lu.item() * mask.sum()\n",
    "\n",
    "            # Total loss\n",
    "            loss = Lx + lambda_u * Lu\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        loss_s = loss_s / (eval_step * batch_size)\n",
    "        loss_u = loss_u / (eval_step * batch_size * ratio)\n",
    "\n",
    "        print(f'Supervised Loss: {loss_s}, Unsupervised Loss: {loss_u}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs_w, inputs_s, labels in val_loader:\n",
    "                inputs = inputs_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_subset)\n",
    "        val_acc = val_corrects.double() / len(test_subset)\n",
    "        print(f'Validation Loss: {val_loss}, Acc: {val_acc}')\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "        \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd9b167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixmatch_active(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1): \n",
    "    model.train()\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_s = 0.0\n",
    "        loss_u = 0.0\n",
    "        for batch_idx in range(eval_step):\n",
    "            try:\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = iter(labeled_loader)\n",
    "                inputs_x, _, targets_x = next(labeled_iter)\n",
    "\n",
    "            try:\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "                if len(inputs_u_w) < batch_size*ratio:\n",
    "                    unlabeled_iter = iter(unlabeled_loader)\n",
    "                    inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = iter(unlabeled_loader)\n",
    "                inputs_u_w, inputs_u_s, _ = next(unlabeled_iter)\n",
    "\n",
    "            inputs_x = inputs_x.to(device)\n",
    "            targets_x = targets_x.to(device)\n",
    "            inputs_u_w = inputs_u_w.to(device)\n",
    "            inputs_u_s = inputs_u_s.to(device)\n",
    "            # print(inputs_x.shape[0], inputs_u_w.shape[0], inputs_u_s.shape[0])\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), ratio*2+1).to(device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, ratio*2+1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            # Compute the loss for labeled data\n",
    "            Lx = criterion(logits_x, targets_x)\n",
    "            loss_s += Lx.item() * inputs_x.size(0)\n",
    "\n",
    "            # Compute the pseudo-labels and loss for unlabeled data\n",
    "            probs = torch.softmax(logits_u_w, dim=-1)\n",
    "            max_probs, targets_u = torch.max(probs, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "\n",
    "            loss_u += Lu.item() * mask.sum()\n",
    "\n",
    "            # Total loss\n",
    "            loss = Lx + lambda_u * Lu\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        loss_s = loss_s / (eval_step * batch_size)\n",
    "        loss_u = loss_u / (eval_step * batch_size * ratio)\n",
    "\n",
    "        # print(f'Supervised Loss: {loss_s}, Unsupervised Loss: {loss_u}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs_w, inputs_s, labels in val_loader:\n",
    "                inputs = inputs_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_subset)\n",
    "        val_acc = val_corrects.double() / len(test_subset)\n",
    "        # print(f'Validation Loss: {val_loss}, Acc: {val_acc}')\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "344ce5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling(model, unlabeled_subset, strategy = 'uncertainty', forward_passes = 100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        if strategy == 'uncertainty':\n",
    "            for _, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "            safe_probabilities = all_predictions.clamp(min=1e-9)\n",
    "            scores = -torch.sum(safe_probabilities * torch.log(safe_probabilities), dim=1)\n",
    "            _, indices = torch.topk(scores, batch_size)\n",
    "        elif strategy == 'diversity':\n",
    "            for inputs_w, _, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "            max_probs, targets_u = torch.max(all_predictions, dim=-1)\n",
    "            mask = max_probs.ge(threshold).bool()\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return indices\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a779bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling(model, unlabeled_subset, strategy = 'uncertainty', forward_passes = 100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        if strategy == 'uncertainty':\n",
    "            all_predictions = []\n",
    "            for _, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "            safe_probabilities = all_predictions.clamp(min=1e-9)\n",
    "            scores = -torch.sum(safe_probabilities * torch.log(safe_probabilities), dim=1)\n",
    "            _, indices = torch.topk(scores, batch_size)\n",
    "        elif strategy == 'diversity':\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            for inputs_w, _, _ in unlabeled_loader:\n",
    "                mean_predictions = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs = inputs_s.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = torch.softmax(outputs, dim=-1)\n",
    "                    mean_predictions.append(probs.unsqueeze(0))\n",
    "                mean_predictions = torch.cat(mean_predictions)\n",
    "                mean_predictions = torch.mean(mean_predictions, 0)\n",
    "                all_predictions.append(mean_predictions)\n",
    "            all_predictions = torch.cat(all_predictions)\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            similarity_matrix = torch.zeros((len(all_predictions), len(all_predictions)))\n",
    "            for i in range(len(all_predictions)):\n",
    "                for j in range(i+1, len(all_predictions)):\n",
    "                    similarity_matrix[i, j] = torch.cosine_similarity(all_predictions[i], all_predictions[j])\n",
    "                    similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "            \n",
    "            # Select the indices with the highest diversity\n",
    "            selected_indices = []\n",
    "            while len(selected_indices) < batch_size:\n",
    "                # Find the index with the lowest average similarity to the already selected indices\n",
    "                min_similarity = float('inf')\n",
    "                min_index = None\n",
    "                for i in range(len(all_predictions)):\n",
    "                    if i not in selected_indices:\n",
    "                        similarity = torch.mean(similarity_matrix[i, selected_indices])\n",
    "                        if similarity < min_similarity:\n",
    "                            min_similarity = similarity\n",
    "                            min_index = i\n",
    "                \n",
    "                # Add the selected index to the list and update the similarity matrix\n",
    "                selected_indices.append(min_index)\n",
    "                similarity_matrix = torch.cat((similarity_matrix[:min_index, :], similarity_matrix[min_index+1:, :]), dim=0)\n",
    "                similarity_matrix = torch.cat((similarity_matrix[:, :min_index], similarity_matrix[:, min_index+1:]), dim=1)\n",
    "        elif strategy == 'consistency':\n",
    "            all_predictions_s = []\n",
    "            all_predictions_w = []\n",
    "            for inputs_w, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions_w = []\n",
    "                mean_predictions_s = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs_s = inputs_s.to(device)\n",
    "                    inputs_w = inputs_w.to(device)\n",
    "                    outputs_s = model(inputs_s)\n",
    "                    outputs_w = model(inputs_w)\n",
    "                    probs_s = torch.softmax(outputs_s, dim=-1)\n",
    "                    probs_w = torch.softmax(outputs_w, dim=-1)\n",
    "                    mean_predictions_s.append(probs_s.unsqueeze(0))\n",
    "                    mean_predictions_w.append(probs_w.unsqueeze(0))\n",
    "                mean_predictions_s = torch.cat(mean_predictions_s)\n",
    "                mean_predictions_s = torch.mean(mean_predictions_s, 0)\n",
    "                mean_predictions_w = torch.cat(mean_predictions_w)\n",
    "                mean_predictions_w = torch.mean(mean_predictions_w, 0)\n",
    "                \n",
    "                all_predictions_s.append(mean_predictions_s)\n",
    "                all_predictions_w.append(mean_predictions_w)\n",
    "            all_predictions_s = torch.cat(all_predictions_s)\n",
    "            all_predictions_w = torch.cat(all_predictions_w)\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            scores = -torch.cosine_similarity(all_predictions_s, all_predictions_w, dim = -1)\n",
    "            _, indices = torch.topk(scores, batch_size)\n",
    "        elif strategy == 'combined':\n",
    "            all_predictions_s = []\n",
    "            all_predictions_w = []\n",
    "            for inputs_w, inputs_s, _ in unlabeled_loader:\n",
    "                mean_predictions_w = []\n",
    "                mean_predictions_s = []\n",
    "                for _ in range(forward_passes):\n",
    "                    inputs_s = inputs_s.to(device)\n",
    "                    inputs_w = inputs_w.to(device)\n",
    "                    outputs_s = model(inputs_s)\n",
    "                    outputs_w = model(inputs_w)\n",
    "                    probs_s = torch.softmax(outputs_s, dim=-1)\n",
    "                    probs_w = torch.softmax(outputs_w, dim=-1)\n",
    "                    mean_predictions_s.append(probs_s.unsqueeze(0))\n",
    "                    mean_predictions_w.append(probs_w.unsqueeze(0))\n",
    "                mean_predictions_s = torch.cat(mean_predictions_s)\n",
    "                mean_predictions_s = torch.mean(mean_predictions_s, 0)\n",
    "                mean_predictions_w = torch.cat(mean_predictions_w)\n",
    "                mean_predictions_w = torch.mean(mean_predictions_w, 0)\n",
    "                \n",
    "                all_predictions_s.append(mean_predictions_s)\n",
    "                all_predictions_w.append(mean_predictions_w)\n",
    "            all_predictions_s = torch.cat(all_predictions_s)\n",
    "            all_predictions_w = torch.cat(all_predictions_w)\n",
    "            # Compute the pairwise similarity between predictions\n",
    "            consistency_scores = torch.cosine_similarity(all_predictions_s, all_predictions_w, dim = -1)\n",
    "            safe_probabilities = all_predictions_s.clamp(min=1e-9)\n",
    "            uncertainty_scores = -torch.sum(safe_probabilities * torch.log(safe_probabilities), dim = 1)\n",
    "            combined_scores = consistency_scores * uncertainty_scores\n",
    "            _, indices = torch.topk(combined_scores, batch_size)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return indices\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e77333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 5.0% of the train data is being used\n",
      "Supervised Loss: 0.9058436810970306, Unsupervised Loss: 0.02819581888616085\n",
      "Validation Loss: 1.3242982771992684, Acc: 0.5760000000000001\n",
      "Supervised Loss: 1.2299780127406121, Unsupervised Loss: 0.0011561702704057097\n",
      "Validation Loss: 0.9649851381778717, Acc: 0.554\n",
      "Supervised Loss: 0.7552391138672828, Unsupervised Loss: 0.0013084320817142725\n",
      "Validation Loss: 0.9268070340156556, Acc: 0.649\n",
      "Supervised Loss: 0.5517346034944057, Unsupervised Loss: 0.01132572628557682\n",
      "Validation Loss: 0.8495236402750015, Acc: 0.736\n",
      "Supervised Loss: 0.5128815933316946, Unsupervised Loss: 0.020450729876756668\n",
      "Validation Loss: 0.8743248873949051, Acc: 0.735\n",
      "Supervised Loss: 0.36257463522255423, Unsupervised Loss: 0.042530592530965805\n",
      "Validation Loss: 0.8804135692119598, Acc: 0.745\n",
      "Supervised Loss: 0.24684018716216088, Unsupervised Loss: 0.08287295699119568\n",
      "Validation Loss: 1.0889705610275269, Acc: 0.734\n",
      "Supervised Loss: 0.17930127911269664, Unsupervised Loss: 0.12142868340015411\n",
      "Validation Loss: 1.1715907192230224, Acc: 0.744\n",
      "Supervised Loss: 0.15943129790946842, Unsupervised Loss: 0.1428787112236023\n",
      "Validation Loss: 1.1335867962241173, Acc: 0.753\n",
      "Supervised Loss: 0.11515379779040813, Unsupervised Loss: 0.17408336699008942\n",
      "Validation Loss: 1.4923641806840897, Acc: 0.745\n",
      "Supervised Loss: 0.11608279882464557, Unsupervised Loss: 0.17312246561050415\n",
      "Validation Loss: 1.5787244041264057, Acc: 0.727\n",
      "Supervised Loss: 0.11768431594595313, Unsupervised Loss: 0.1743890941143036\n",
      "Validation Loss: 1.1894931906461716, Acc: 0.759\n",
      "Supervised Loss: 0.08534297795034945, Unsupervised Loss: 0.18887493014335632\n",
      "Validation Loss: 1.1756003032624722, Acc: 0.762\n",
      "Supervised Loss: 0.07526851798640564, Unsupervised Loss: 0.2004610002040863\n",
      "Validation Loss: 1.5545296063274145, Acc: 0.733\n",
      "Supervised Loss: 0.05280953248031437, Unsupervised Loss: 0.19330057501792908\n",
      "Validation Loss: 1.0893601787835359, Acc: 0.766\n",
      "Supervised Loss: 0.05742301830789074, Unsupervised Loss: 0.20433342456817627\n",
      "Validation Loss: 1.5094169434905051, Acc: 0.765\n",
      "Supervised Loss: 0.04115381973097101, Unsupervised Loss: 0.21044355630874634\n",
      "Validation Loss: 1.2411203906312585, Acc: 0.745\n",
      "Supervised Loss: 0.07290247689001263, Unsupervised Loss: 0.19411563873291016\n",
      "Validation Loss: 1.4008071237802506, Acc: 0.737\n",
      "Supervised Loss: 0.059409176188055426, Unsupervised Loss: 0.2050352245569229\n",
      "Validation Loss: 1.4031705582141876, Acc: 0.734\n",
      "Supervised Loss: 0.038982492783106865, Unsupervised Loss: 0.20788690447807312\n",
      "Validation Loss: 1.173295531272888, Acc: 0.768\n",
      "Supervised Loss: 0.04789104111783672, Unsupervised Loss: 0.20109397172927856\n",
      "Validation Loss: 1.1623634833097458, Acc: 0.784\n",
      "Supervised Loss: 0.03568855543853715, Unsupervised Loss: 0.19770468771457672\n",
      "Validation Loss: 1.1826183132827282, Acc: 0.786\n",
      "Supervised Loss: 0.05465861805947497, Unsupervised Loss: 0.19696857035160065\n",
      "Validation Loss: 1.17836223192513, Acc: 0.787\n",
      "Supervised Loss: 0.029780346330953764, Unsupervised Loss: 0.19679036736488342\n",
      "Validation Loss: 1.3942313868179916, Acc: 0.766\n",
      "Supervised Loss: 0.030947519352776, Unsupervised Loss: 0.1991378515958786\n",
      "Validation Loss: 1.406463125348091, Acc: 0.771\n",
      "Supervised Loss: 0.07680106967105531, Unsupervised Loss: 0.18741898238658905\n",
      "Validation Loss: 1.365125536620617, Acc: 0.765\n",
      "Supervised Loss: 0.059928929317975416, Unsupervised Loss: 0.1985102891921997\n",
      "Validation Loss: 1.4320548829436301, Acc: 0.766\n",
      "Supervised Loss: 0.06529210683540441, Unsupervised Loss: 0.1853971779346466\n",
      "Validation Loss: 1.187257398068905, Acc: 0.765\n",
      "Supervised Loss: 0.03514120447449386, Unsupervised Loss: 0.19649459421634674\n",
      "Validation Loss: 1.4875077295303345, Acc: 0.751\n",
      "best_acc:0.787\n",
      "Currently 6.0% of the train data is being used\n",
      "Supervised Loss: 0.9367819398641586, Unsupervised Loss: 0.013704107142984867\n",
      "Validation Loss: 1.3826299262046815, Acc: 0.5650000000000001\n",
      "Supervised Loss: 1.4372485145926475, Unsupervised Loss: 0.0012442426523193717\n",
      "Validation Loss: 0.9480050897598267, Acc: 0.626\n",
      "Supervised Loss: 0.8983822599053383, Unsupervised Loss: 0.00012451477232389152\n",
      "Validation Loss: 0.8420209681987763, Acc: 0.672\n",
      "Supervised Loss: 0.7475353910028935, Unsupervised Loss: 0.0035211879294365644\n",
      "Validation Loss: 0.9607446485757828, Acc: 0.587\n",
      "Supervised Loss: 0.6370214353501796, Unsupervised Loss: 0.008267280645668507\n",
      "Validation Loss: 0.9462949621677399, Acc: 0.595\n",
      "Supervised Loss: 0.5197269500046968, Unsupervised Loss: 0.01586751639842987\n",
      "Validation Loss: 0.9259634217619896, Acc: 0.6960000000000001\n",
      "Supervised Loss: 0.4075271517038345, Unsupervised Loss: 0.036491863429546356\n",
      "Validation Loss: 1.094994853436947, Acc: 0.71\n",
      "Supervised Loss: 0.3780642995238304, Unsupervised Loss: 0.04912053421139717\n",
      "Validation Loss: 0.7888492619991303, Acc: 0.768\n",
      "Supervised Loss: 0.2612818527594209, Unsupervised Loss: 0.0989510789513588\n",
      "Validation Loss: 0.9844123816490173, Acc: 0.72\n",
      "Supervised Loss: 0.2110807159356773, Unsupervised Loss: 0.13228759169578552\n",
      "Validation Loss: 0.9192761304974556, Acc: 0.764\n",
      "Supervised Loss: 0.22669508123770357, Unsupervised Loss: 0.14333795011043549\n",
      "Validation Loss: 0.8902684622257948, Acc: 0.756\n",
      "Supervised Loss: 0.14735681382007898, Unsupervised Loss: 0.16791057586669922\n",
      "Validation Loss: 1.0817379549518227, Acc: 0.733\n",
      "Supervised Loss: 0.13487255297601222, Unsupervised Loss: 0.18115046620368958\n",
      "Validation Loss: 0.9049530848860741, Acc: 0.799\n",
      "Supervised Loss: 0.10802150196395814, Unsupervised Loss: 0.19766442477703094\n",
      "Validation Loss: 1.0099098774790765, Acc: 0.787\n",
      "Supervised Loss: 0.0921173553634435, Unsupervised Loss: 0.20393794775009155\n",
      "Validation Loss: 1.1120055648684501, Acc: 0.771\n",
      "Supervised Loss: 0.08445493566803634, Unsupervised Loss: 0.19687184691429138\n",
      "Validation Loss: 0.90573900654912, Acc: 0.789\n",
      "Supervised Loss: 0.07908137778751552, Unsupervised Loss: 0.20997101068496704\n",
      "Validation Loss: 0.96003387093544, Acc: 0.8130000000000001\n",
      "Supervised Loss: 0.060203387739602474, Unsupervised Loss: 0.20777292549610138\n",
      "Validation Loss: 1.0030945042520762, Acc: 0.809\n",
      "Supervised Loss: 0.11852250348310918, Unsupervised Loss: 0.1922362744808197\n",
      "Validation Loss: 0.8936132861673832, Acc: 0.797\n",
      "Supervised Loss: 0.10001582393888384, Unsupervised Loss: 0.18877306580543518\n",
      "Validation Loss: 1.0358833661675453, Acc: 0.783\n",
      "Supervised Loss: 0.051830950421281156, Unsupervised Loss: 0.21066346764564514\n",
      "Validation Loss: 0.9041745821386575, Acc: 0.804\n",
      "Supervised Loss: 0.0359895585780032, Unsupervised Loss: 0.20501649379730225\n",
      "Validation Loss: 0.92157509714365, Acc: 0.8130000000000001\n",
      "Supervised Loss: 0.035902241099392995, Unsupervised Loss: 0.20922008156776428\n",
      "Validation Loss: 1.1672789515927433, Acc: 0.8\n",
      "Supervised Loss: 0.05223121355171315, Unsupervised Loss: 0.20727233588695526\n",
      "Validation Loss: 0.970323421061039, Acc: 0.807\n",
      "Supervised Loss: 0.07229448699159548, Unsupervised Loss: 0.1924704611301422\n",
      "Validation Loss: 0.9095746380090713, Acc: 0.804\n",
      "Supervised Loss: 0.03129388457979076, Unsupervised Loss: 0.206956684589386\n",
      "Validation Loss: 1.0372479857504369, Acc: 0.801\n",
      "Supervised Loss: 0.02122414513141848, Unsupervised Loss: 0.2056041657924652\n",
      "Validation Loss: 1.005202316045761, Acc: 0.797\n",
      "Supervised Loss: 0.04232716333935969, Unsupervised Loss: 0.20291222631931305\n",
      "Validation Loss: 0.9497176424320787, Acc: 0.796\n",
      "Supervised Loss: 0.038866084340261293, Unsupervised Loss: 0.202461838722229\n",
      "Validation Loss: 0.9857300269603729, Acc: 0.8130000000000001\n",
      "Supervised Loss: 0.07916141212219373, Unsupervised Loss: 0.19704890251159668\n",
      "Validation Loss: 0.8258058202266693, Acc: 0.803\n",
      "best_acc:0.8130000000000001\n",
      "Currently 7.000000000000001% of the train data is being used\n",
      "Supervised Loss: 1.028556362837553, Unsupervised Loss: 0.01639976166188717\n",
      "Validation Loss: 1.256115899682045, Acc: 0.5670000000000001\n",
      "Supervised Loss: 1.8643914145231246, Unsupervised Loss: 0.002805998781695962\n",
      "Validation Loss: 1.1102139472961425, Acc: 0.597\n",
      "Supervised Loss: 1.0216907361149787, Unsupervised Loss: 2.438300543872174e-05\n",
      "Validation Loss: 0.9661913597583771, Acc: 0.557\n",
      "Supervised Loss: 0.8235533574223518, Unsupervised Loss: 0.0015679029747843742\n",
      "Validation Loss: 0.7606138479709625, Acc: 0.704\n",
      "Supervised Loss: 0.6091449227929115, Unsupervised Loss: 0.01225963607430458\n",
      "Validation Loss: 0.7059111905097961, Acc: 0.753\n",
      "Supervised Loss: 0.5184794782102108, Unsupervised Loss: 0.02839360199868679\n",
      "Validation Loss: 0.7670121693611145, Acc: 0.723\n",
      "Supervised Loss: 0.40898292444646356, Unsupervised Loss: 0.04016238451004028\n",
      "Validation Loss: 0.8561902242898941, Acc: 0.717\n",
      "Supervised Loss: 0.52247315376997, Unsupervised Loss: 0.045416124165058136\n",
      "Validation Loss: 0.7790772086381912, Acc: 0.754\n",
      "Supervised Loss: 0.3337577722966671, Unsupervised Loss: 0.06501877307891846\n",
      "Validation Loss: 0.8939023524522781, Acc: 0.731\n",
      "Supervised Loss: 0.2919739091023803, Unsupervised Loss: 0.08353620767593384\n",
      "Validation Loss: 0.8456337481737137, Acc: 0.768\n",
      "Supervised Loss: 0.20890227938070893, Unsupervised Loss: 0.12219250202178955\n",
      "Validation Loss: 0.9874363957345486, Acc: 0.748\n",
      "Supervised Loss: 0.15998194282874464, Unsupervised Loss: 0.14954964816570282\n",
      "Validation Loss: 0.9292647933959961, Acc: 0.756\n",
      "Supervised Loss: 0.1468639373779297, Unsupervised Loss: 0.17143547534942627\n",
      "Validation Loss: 0.9471048700809479, Acc: 0.758\n",
      "Supervised Loss: 0.10904985275119543, Unsupervised Loss: 0.1956469863653183\n",
      "Validation Loss: 0.7519497340917587, Acc: 0.767\n",
      "Supervised Loss: 0.09238101764582098, Unsupervised Loss: 0.19124087691307068\n",
      "Validation Loss: 0.8266602744162083, Acc: 0.79\n",
      "Supervised Loss: 0.09337738468311726, Unsupervised Loss: 0.19611048698425293\n",
      "Validation Loss: 0.8929708229750395, Acc: 0.784\n",
      "Supervised Loss: 0.09589613177347928, Unsupervised Loss: 0.20458222925662994\n",
      "Validation Loss: 0.9736551605165005, Acc: 0.787\n",
      "Supervised Loss: 0.07623399883508682, Unsupervised Loss: 0.21052587032318115\n",
      "Validation Loss: 0.9015583227947355, Acc: 0.798\n",
      "Supervised Loss: 0.05998405311722308, Unsupervised Loss: 0.22572104632854462\n",
      "Validation Loss: 0.8715857286565005, Acc: 0.799\n",
      "Supervised Loss: 0.05661715724039823, Unsupervised Loss: 0.21454347670078278\n",
      "Validation Loss: 1.0001247149333359, Acc: 0.789\n",
      "Supervised Loss: 0.0771152307651937, Unsupervised Loss: 0.20671036839485168\n",
      "Validation Loss: 0.9259904524683953, Acc: 0.783\n",
      "Supervised Loss: 0.051838106540963054, Unsupervised Loss: 0.2232399582862854\n",
      "Validation Loss: 1.1079753526346758, Acc: 0.788\n",
      "Supervised Loss: 0.04166495185578242, Unsupervised Loss: 0.2169080227613449\n",
      "Validation Loss: 0.9521121747605502, Acc: 0.804\n",
      "Supervised Loss: 0.04365298158256337, Unsupervised Loss: 0.21273700892925262\n",
      "Validation Loss: 0.9454552382603287, Acc: 0.81\n",
      "Supervised Loss: 0.05588191871065647, Unsupervised Loss: 0.21469128131866455\n",
      "Validation Loss: 0.9524002547934651, Acc: 0.753\n",
      "Supervised Loss: 0.04508749990258366, Unsupervised Loss: 0.2055387943983078\n",
      "Validation Loss: 0.9430145637691021, Acc: 0.79\n",
      "Supervised Loss: 0.042957792463712396, Unsupervised Loss: 0.21201354265213013\n",
      "Validation Loss: 0.8909602933377028, Acc: 0.808\n",
      "Supervised Loss: 0.04546200444223359, Unsupervised Loss: 0.20301569998264313\n",
      "Validation Loss: 0.8902826601779088, Acc: 0.805\n",
      "Supervised Loss: 0.0339717575465329, Unsupervised Loss: 0.21186113357543945\n",
      "Validation Loss: 0.9637597777001793, Acc: 0.8210000000000001\n",
      "Supervised Loss: 0.031686643733410166, Unsupervised Loss: 0.20697557926177979\n",
      "Validation Loss: 0.950796866826713, Acc: 0.8180000000000001\n",
      "best_acc:0.8210000000000001\n",
      "Currently 8.0% of the train data is being used\n",
      "Supervised Loss: 0.9952879798412323, Unsupervised Loss: 0.02687133476138115\n",
      "Validation Loss: 1.4975532925128936, Acc: 0.511\n",
      "Supervised Loss: 1.8560312110185624, Unsupervised Loss: 0.003116674954071641\n",
      "Validation Loss: 1.0753210401535034, Acc: 0.504\n",
      "Supervised Loss: 1.052800567448139, Unsupervised Loss: 2.221215424924594e-07\n",
      "Validation Loss: 0.8354239678382873, Acc: 0.604\n",
      "Supervised Loss: 0.8606387171149253, Unsupervised Loss: 0.0016287568723782897\n",
      "Validation Loss: 0.8397865045070648, Acc: 0.635\n",
      "Supervised Loss: 0.741276639699936, Unsupervised Loss: 0.007160836830735207\n",
      "Validation Loss: 0.703180136680603, Acc: 0.764\n",
      "Supervised Loss: 0.6350884653627873, Unsupervised Loss: 0.015872661024332047\n",
      "Validation Loss: 0.6911144542694092, Acc: 0.773\n",
      "Supervised Loss: 0.5970066122710704, Unsupervised Loss: 0.01972905360162258\n",
      "Validation Loss: 0.6575866603851318, Acc: 0.775\n",
      "Supervised Loss: 0.5487930412590504, Unsupervised Loss: 0.025506146252155304\n",
      "Validation Loss: 0.9797438492625952, Acc: 0.634\n",
      "Supervised Loss: 0.535575616210699, Unsupervised Loss: 0.02964230254292488\n",
      "Validation Loss: 0.7240996432304382, Acc: 0.727\n",
      "Supervised Loss: 0.4638214707374573, Unsupervised Loss: 0.03671800345182419\n",
      "Validation Loss: 0.7940283852815628, Acc: 0.722\n",
      "Supervised Loss: 0.4493889879435301, Unsupervised Loss: 0.040117811411619186\n",
      "Validation Loss: 0.7999407005310059, Acc: 0.716\n",
      "Supervised Loss: 0.3909735614806414, Unsupervised Loss: 0.062593013048172\n",
      "Validation Loss: 0.8901870673894883, Acc: 0.717\n",
      "Supervised Loss: 0.35623859260231255, Unsupervised Loss: 0.0716937780380249\n",
      "Validation Loss: 0.8317521238327026, Acc: 0.779\n",
      "Supervised Loss: 0.2763657119125128, Unsupervised Loss: 0.09735548496246338\n",
      "Validation Loss: 0.9464841255545616, Acc: 0.737\n",
      "Supervised Loss: 0.27074493519961834, Unsupervised Loss: 0.1078575998544693\n",
      "Validation Loss: 0.8203481718897819, Acc: 0.755\n",
      "Supervised Loss: 0.27163361076265574, Unsupervised Loss: 0.10302630066871643\n",
      "Validation Loss: 0.881482760310173, Acc: 0.747\n",
      "Supervised Loss: 0.19807397563010454, Unsupervised Loss: 0.14646601676940918\n",
      "Validation Loss: 1.0354781609773636, Acc: 0.762\n",
      "Supervised Loss: 0.22879701111465692, Unsupervised Loss: 0.13674665987491608\n",
      "Validation Loss: 0.7915711888670921, Acc: 0.774\n",
      "Supervised Loss: 0.18153928760439159, Unsupervised Loss: 0.14364153146743774\n",
      "Validation Loss: 0.9687708170711994, Acc: 0.768\n",
      "Supervised Loss: 0.16647044816985726, Unsupervised Loss: 0.15865814685821533\n",
      "Validation Loss: 1.1761938971281052, Acc: 0.761\n",
      "Supervised Loss: 0.14878440850414335, Unsupervised Loss: 0.16570362448692322\n",
      "Validation Loss: 1.126327388882637, Acc: 0.757\n",
      "Supervised Loss: 0.13945622806437313, Unsupervised Loss: 0.16953037679195404\n",
      "Validation Loss: 0.9324908220767975, Acc: 0.784\n",
      "Supervised Loss: 0.12411739243194461, Unsupervised Loss: 0.17970968782901764\n",
      "Validation Loss: 1.149989946782589, Acc: 0.761\n",
      "Supervised Loss: 0.14027689296752213, Unsupervised Loss: 0.18745417892932892\n",
      "Validation Loss: 0.8730494210124016, Acc: 0.761\n",
      "Supervised Loss: 0.1250627444963902, Unsupervised Loss: 0.1726759672164917\n",
      "Validation Loss: 0.8136807908117771, Acc: 0.788\n",
      "Supervised Loss: 0.09641078161075711, Unsupervised Loss: 0.193870410323143\n",
      "Validation Loss: 0.890185100287199, Acc: 0.788\n",
      "Supervised Loss: 0.1080502019636333, Unsupervised Loss: 0.19180060923099518\n",
      "Validation Loss: 1.2494523337483405, Acc: 0.773\n",
      "Supervised Loss: 0.1171665073139593, Unsupervised Loss: 0.18130932748317719\n",
      "Validation Loss: 1.1459501990675927, Acc: 0.769\n",
      "Supervised Loss: 0.08074910377617925, Unsupervised Loss: 0.19961005449295044\n",
      "Validation Loss: 1.1679719215258957, Acc: 0.767\n",
      "Supervised Loss: 0.08453647361602634, Unsupervised Loss: 0.19470499455928802\n",
      "Validation Loss: 0.9561738144606352, Acc: 0.775\n",
      "best_acc:0.788\n",
      "Currently 9.0% of the train data is being used\n",
      "Supervised Loss: 1.2074168106913568, Unsupervised Loss: 0.005061035975813866\n",
      "Validation Loss: 1.1952262544631957, Acc: 0.506\n",
      "Supervised Loss: 1.1362605318427086, Unsupervised Loss: 0.0001492168812546879\n",
      "Validation Loss: 0.8490947997570037, Acc: 0.627\n",
      "Supervised Loss: 0.7870315404236317, Unsupervised Loss: 0.0007198220118880272\n",
      "Validation Loss: 0.6745023214817047, Acc: 0.722\n",
      "Supervised Loss: 0.6945117837190629, Unsupervised Loss: 0.006069015245884657\n",
      "Validation Loss: 0.6870012867450714, Acc: 0.728\n",
      "Supervised Loss: 0.5651404967904091, Unsupervised Loss: 0.021123303100466728\n",
      "Validation Loss: 0.7239849543571473, Acc: 0.739\n",
      "Supervised Loss: 0.48944817468523977, Unsupervised Loss: 0.04212117940187454\n",
      "Validation Loss: 0.8127726628631353, Acc: 0.756\n",
      "Supervised Loss: 0.42339090786874295, Unsupervised Loss: 0.06259027123451233\n",
      "Validation Loss: 0.850541969537735, Acc: 0.709\n",
      "Supervised Loss: 0.3479353272914886, Unsupervised Loss: 0.07892872393131256\n",
      "Validation Loss: 0.6803994917869568, Acc: 0.793\n",
      "Supervised Loss: 0.29809959784150125, Unsupervised Loss: 0.0925283282995224\n",
      "Validation Loss: 1.0066867080330848, Acc: 0.762\n",
      "Supervised Loss: 0.2525177900865674, Unsupervised Loss: 0.1250590682029724\n",
      "Validation Loss: 0.8849867127835751, Acc: 0.755\n",
      "Supervised Loss: 0.2084731741808355, Unsupervised Loss: 0.1502436101436615\n",
      "Validation Loss: 0.8494147764146328, Acc: 0.786\n",
      "Supervised Loss: 0.19489228105172515, Unsupervised Loss: 0.15950173139572144\n",
      "Validation Loss: 0.9722201064229011, Acc: 0.773\n",
      "Supervised Loss: 0.14837704794481396, Unsupervised Loss: 0.1764911264181137\n",
      "Validation Loss: 0.9725722893327474, Acc: 0.79\n",
      "Supervised Loss: 0.1516883004363626, Unsupervised Loss: 0.17281727492809296\n",
      "Validation Loss: 0.936421599984169, Acc: 0.794\n",
      "Supervised Loss: 0.1095361856603995, Unsupervised Loss: 0.20563539862632751\n",
      "Validation Loss: 0.9396010833978653, Acc: 0.803\n",
      "Supervised Loss: 0.09844572738278658, Unsupervised Loss: 0.2054292857646942\n",
      "Validation Loss: 1.0144535301625728, Acc: 0.795\n",
      "Supervised Loss: 0.08150046471972018, Unsupervised Loss: 0.2093350738286972\n",
      "Validation Loss: 0.9421923279762268, Acc: 0.784\n",
      "Supervised Loss: 0.06282588948262856, Unsupervised Loss: 0.21224789321422577\n",
      "Validation Loss: 0.8055149180814624, Acc: 0.809\n",
      "Supervised Loss: 0.10173752351198345, Unsupervised Loss: 0.2011992186307907\n",
      "Validation Loss: 1.0087062191963196, Acc: 0.8\n",
      "Supervised Loss: 0.08021050322800875, Unsupervised Loss: 0.19885781407356262\n",
      "Validation Loss: 0.8794840769283474, Acc: 0.801\n",
      "Supervised Loss: 0.05493462954182178, Unsupervised Loss: 0.21131527423858643\n",
      "Validation Loss: 0.9622414723038674, Acc: 0.8150000000000001\n",
      "Supervised Loss: 0.06574387328932062, Unsupervised Loss: 0.2037205696105957\n",
      "Validation Loss: 0.7737982511520386, Acc: 0.808\n",
      "Supervised Loss: 0.0603874721378088, Unsupervised Loss: 0.2090822160243988\n",
      "Validation Loss: 0.8895942150428892, Acc: 0.809\n",
      "Supervised Loss: 0.04092978839296848, Unsupervised Loss: 0.2049552947282791\n",
      "Validation Loss: 0.893780738376081, Acc: 0.8210000000000001\n",
      "Supervised Loss: 0.05473215350415558, Unsupervised Loss: 0.20195387303829193\n",
      "Validation Loss: 0.9381963200401515, Acc: 0.8140000000000001\n",
      "Supervised Loss: 0.050263778211083264, Unsupervised Loss: 0.2003169059753418\n",
      "Validation Loss: 0.7959922694414854, Acc: 0.8240000000000001\n",
      "Supervised Loss: 0.05208995329448953, Unsupervised Loss: 0.19902187585830688\n",
      "Validation Loss: 0.8283560481294989, Acc: 0.8310000000000001\n",
      "Supervised Loss: 0.0637276519741863, Unsupervised Loss: 0.20359373092651367\n",
      "Validation Loss: 0.9985606426000595, Acc: 0.794\n",
      "Supervised Loss: 0.03974208861705847, Unsupervised Loss: 0.2052580714225769\n",
      "Validation Loss: 0.8652558843791485, Acc: 0.8270000000000001\n",
      "Supervised Loss: 0.047402480214368554, Unsupervised Loss: 0.20331957936286926\n",
      "Validation Loss: 0.8486047895113006, Acc: 0.8260000000000001\n",
      "best_acc:0.8310000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 10):\n",
    "    set_seed(seed)\n",
    "    model = CRCClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    curr_train_data_portion = 0.01*i\n",
    "    print(f'Currently {curr_train_data_portion*100}% of the train data is being used')\n",
    "    if i == 5:\n",
    "        unlabeled_subset, labeled_subset = stratified_split(train_subset, test_size=curr_train_data_portion)\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "        acc = train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)\n",
    "        #torch.save(model.state_dict(), 'semi_5_percents_combined.pth')\n",
    "        indices = active_sampling(model, unlabeled_subset, 'combined')\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    else:\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "        acc = train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)\n",
    "        indices = active_sampling(model, unlabeled_subset, 'combined')\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    print(f'best_acc:{acc}')\n",
    "    with open('semi_val_accs_act_combined_0.05_3.txt','a+') as f:\n",
    "        f.write(f'{acc}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "303aefc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1031, 1769, 1320,  328, 1542,  261, 1138,  676, 1102, 1325, 1861, 1335,\n",
       "        2445, 1459, 1880, 1434, 1692, 1717,  304,   81,  400, 1748, 1255,  468,\n",
       "        1609, 1505, 1354, 1697,  246, 1867, 1132, 1148,  568,  311, 1254,  969,\n",
       "        1016, 1204, 1078,  611], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_sampling(model, unlabeled_subset, 'consistency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "418bc1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 5.0% of the train data is being used\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mtrain_fixmatch_multistep\u001b[0;34m(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     inputs_x, _, targets_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabeled_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1306\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_workers()\n\u001b[0;32m-> 1306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m labeled_loader \u001b[38;5;241m=\u001b[39m DataLoader(labeled_subset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m unlabeled_loader \u001b[38;5;241m=\u001b[39m DataLoader(unlabeled_subset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39mratio, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fixmatch_multistep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_u\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemi_5_percents.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m indices \u001b[38;5;241m=\u001b[39m active_sampling(model, unlabeled_subset)\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mtrain_fixmatch_multistep\u001b[0;34m(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     labeled_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(labeled_loader)\n\u001b[0;32m---> 14\u001b[0m     inputs_x, _, targets_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabeled_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     inputs_u_w, inputs_u_s, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(unlabeled_iter)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5, 10):\n",
    "    model = CRCClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    curr_train_data_portion = 0.01*i\n",
    "    print(f'Currently {curr_train_data_portion*100}% of the train data is being used')\n",
    "    if i == 5:\n",
    "        unlabeled_subset, labeled_subset = stratified_split(train_subset, test_size=curr_train_data_portion)\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "        acc = train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)\n",
    "        torch.save(model.state_dict(), 'semi_5_percents.pth')\n",
    "        indices = active_sampling(model, unlabeled_subset)\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    else:\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        unlabeled_loader = DataLoader(unlabeled_subset, batch_size=batch_size*ratio, shuffle=True, num_workers=2)\n",
    "        acc = train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)\n",
    "        indices = active_sampling(model, unlabeled_subset)\n",
    "        labeled_subset = ConcatDataset([labeled_subset,Subset(unlabeled_subset, indices.tolist())])\n",
    "        all_indices = torch.arange(len(unlabeled_subset)).to(device)\n",
    "        mask = ~torch.isin(all_indices, indices)\n",
    "        indices_to_keep = all_indices[mask]\n",
    "        unlabeled_subset = Subset(dataset, indices_to_keep.tolist())\n",
    "    print(f'best_acc:{acc}')\n",
    "    with open('semi_val_accs_act_0.05.txt','a+') as f:\n",
    "        f.write(f'{acc}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c04b02c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 41.0% of the train data is being used\n",
      "Supervised Loss: 1.2811084246635438, Unsupervised Loss: 0.006343635264784098\n",
      "Validation Loss: 3.032578038573265, Acc: 0.387\n",
      "Supervised Loss: 1.1823661908507348, Unsupervised Loss: 0.0006406180909834802\n",
      "Validation Loss: 0.9089127492904663, Acc: 0.579\n",
      "Supervised Loss: 0.7857964646816253, Unsupervised Loss: 0.0019456836162135005\n",
      "Validation Loss: 0.6994425225257873, Acc: 0.738\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(curr_train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fixmatch_multistep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_u\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_acc:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemi_val_accs_stratified_1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[19], line 42\u001b[0m, in \u001b[0;36mtrain_fixmatch_multistep\u001b[0;34m(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Compute the loss for labeled data\u001b[39;00m\n\u001b[1;32m     41\u001b[0m Lx \u001b[38;5;241m=\u001b[39m criterion(logits_x, targets_x)\n\u001b[0;32m---> 42\u001b[0m loss_s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mLx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs_x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Compute the pseudo-labels and loss for unlabeled data\u001b[39;00m\n\u001b[1;32m     45\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits_u_w, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_percentage = 0.01\n",
    "for i in range(41,51):\n",
    "    model = CRCClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    curr_train_data_portion = start_percentage*i\n",
    "    print(f'Currently {curr_train_data_portion*100}% of the train data is being used')\n",
    "    curr_train_data_len = int(len(train_subset)*curr_train_data_portion)\n",
    "    indices = torch.randperm(len(train_subset))[:curr_train_data_len]\n",
    "    curr_train_data = Subset(train_subset, indices)\n",
    "    train_loader = DataLoader(curr_train_data, batch_size=40, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(test_subset, batch_size=40, shuffle=False)\n",
    "    acc = train_fixmatch_multistep(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)\n",
    "    print(f'best_acc:{acc}')\n",
    "    with open('semi_val_accs_stratified_1.txt','a+') as f:\n",
    "        f.write(f'{acc}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ab1090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Loss: 0.8793807696551085, Unsupervised Loss: 0.022424958646297455\n",
      "Validation Loss: 1.616305254875806, Acc: 0.581\n",
      "Supervised Loss: 1.4238559740781784, Unsupervised Loss: 0.00122667639516294\n",
      "Validation Loss: 0.8330651044845581, Acc: 0.68\n",
      "Supervised Loss: 0.8244266611337662, Unsupervised Loss: 0.00022006094513926655\n",
      "Validation Loss: 0.740202261209488, Acc: 0.715\n",
      "Supervised Loss: 0.7158406937122345, Unsupervised Loss: 0.002668942091986537\n",
      "Validation Loss: 1.06941114962101, Acc: 0.6\n",
      "Supervised Loss: 0.6107849794626236, Unsupervised Loss: 0.007886268198490143\n",
      "Validation Loss: 0.7874435633420944, Acc: 0.722\n",
      "Supervised Loss: 0.5477163337171078, Unsupervised Loss: 0.013094935566186905\n",
      "Validation Loss: 0.7109185874462127, Acc: 0.731\n",
      "Supervised Loss: 0.4829804074764252, Unsupervised Loss: 0.026563582941889763\n",
      "Validation Loss: 0.7224354898929596, Acc: 0.75\n",
      "Supervised Loss: 0.42736384078860284, Unsupervised Loss: 0.039599817246198654\n",
      "Validation Loss: 0.7682253640890121, Acc: 0.768\n",
      "Supervised Loss: 0.34974340111017227, Unsupervised Loss: 0.06305999308824539\n",
      "Validation Loss: 0.7419093698263168, Acc: 0.756\n",
      "Supervised Loss: 0.32338019195944073, Unsupervised Loss: 0.07844983786344528\n",
      "Validation Loss: 0.7919049084186554, Acc: 0.76\n",
      "Supervised Loss: 0.28280438888818027, Unsupervised Loss: 0.09122037142515182\n",
      "Validation Loss: 0.7166391694545746, Acc: 0.776\n",
      "Supervised Loss: 0.24426709491759538, Unsupervised Loss: 0.11521577090024948\n",
      "Validation Loss: 0.6903930680453777, Acc: 0.803\n",
      "Supervised Loss: 0.18940540814772247, Unsupervised Loss: 0.14490900933742523\n",
      "Validation Loss: 0.6608986835181713, Acc: 0.806\n",
      "Supervised Loss: 0.16537701312452555, Unsupervised Loss: 0.16246697306632996\n",
      "Validation Loss: 0.7545484687387943, Acc: 0.807\n",
      "Supervised Loss: 0.176050261920318, Unsupervised Loss: 0.159327894449234\n",
      "Validation Loss: 0.7941861289739609, Acc: 0.804\n",
      "Supervised Loss: 0.13071042662486435, Unsupervised Loss: 0.16443757712841034\n",
      "Validation Loss: 0.7906576061248779, Acc: 0.807\n",
      "Supervised Loss: 0.10420461679110303, Unsupervised Loss: 0.19092866778373718\n",
      "Validation Loss: 0.7149235780537129, Acc: 0.8280000000000001\n",
      "Supervised Loss: 0.11104460255242884, Unsupervised Loss: 0.1714136004447937\n",
      "Validation Loss: 0.8115673835575581, Acc: 0.811\n",
      "Supervised Loss: 0.09748186204116792, Unsupervised Loss: 0.19282588362693787\n",
      "Validation Loss: 0.8545788279175759, Acc: 0.807\n",
      "Supervised Loss: 0.09437523166649044, Unsupervised Loss: 0.18952710926532745\n",
      "Validation Loss: 0.7905163641273976, Acc: 0.8300000000000001\n",
      "Supervised Loss: 0.11053173230495304, Unsupervised Loss: 0.17542429268360138\n",
      "Validation Loss: 0.7578305813670159, Acc: 0.8160000000000001\n",
      "Supervised Loss: 0.19181898911017925, Unsupervised Loss: 0.15639545023441315\n",
      "Validation Loss: 0.7821792742609978, Acc: 0.8170000000000001\n",
      "Supervised Loss: 0.1266191491484642, Unsupervised Loss: 0.16541124880313873\n",
      "Validation Loss: 0.7711802524328232, Acc: 0.8210000000000001\n",
      "Supervised Loss: 0.118316207556054, Unsupervised Loss: 0.16138403117656708\n",
      "Validation Loss: 0.8281203788518906, Acc: 0.809\n",
      "Supervised Loss: 0.09175911301746964, Unsupervised Loss: 0.17769044637680054\n",
      "Validation Loss: 0.7669613014161587, Acc: 0.8190000000000001\n",
      "Supervised Loss: 0.10044233762193472, Unsupervised Loss: 0.1724214404821396\n",
      "Validation Loss: 0.9369191014766693, Acc: 0.803\n",
      "Supervised Loss: 0.09549633778631687, Unsupervised Loss: 0.15220820903778076\n",
      "Validation Loss: 0.7964393642544746, Acc: 0.8220000000000001\n",
      "Supervised Loss: 0.10011623301543296, Unsupervised Loss: 0.15610364079475403\n",
      "Validation Loss: 0.9301471020653844, Acc: 0.8140000000000001\n",
      "Supervised Loss: 0.09299662992823869, Unsupervised Loss: 0.1869307905435562\n",
      "Validation Loss: 0.7560555039346218, Acc: 0.809\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_fixmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_u\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m, in \u001b[0;36mtrain_fixmatch\u001b[0;34m(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u)\u001b[0m\n\u001b[1;32m     28\u001b[0m     unlabeled_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(unlabeled_loader)\n\u001b[1;32m     29\u001b[0m     inputs_u_w, inputs_u_s, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(unlabeled_iter)\n\u001b[0;32m---> 31\u001b[0m inputs_x \u001b[38;5;241m=\u001b[39m \u001b[43minputs_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m targets_x \u001b[38;5;241m=\u001b[39m targets_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m inputs_u_w \u001b[38;5;241m=\u001b[39m inputs_u_w\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf847260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Loss: 1.0134291757643223, Unsupervised Loss: 0.025716563686728477\n",
      "Validation Loss: 1.1932102400064468, Acc: 0.667\n",
      "Supervised Loss: 1.311743875145912, Unsupervised Loss: 0.001087491400539875\n",
      "Validation Loss: 0.9457165050506592, Acc: 0.587\n",
      "Supervised Loss: 0.7710262385010719, Unsupervised Loss: 0.001857718569226563\n",
      "Validation Loss: 1.0982925128936767, Acc: 0.539\n",
      "Supervised Loss: 0.639760956466198, Unsupervised Loss: 0.008201740682125092\n",
      "Validation Loss: 0.7381910591199994, Acc: 0.721\n",
      "Supervised Loss: 0.4721998928487301, Unsupervised Loss: 0.03007802926003933\n",
      "Validation Loss: 0.8140553459525108, Acc: 0.714\n",
      "Supervised Loss: 0.34529198694974184, Unsupervised Loss: 0.062282633036375046\n",
      "Validation Loss: 0.9334692762233316, Acc: 0.705\n",
      "Supervised Loss: 0.24022329680621624, Unsupervised Loss: 0.09683683514595032\n",
      "Validation Loss: 0.8809191972389817, Acc: 0.761\n",
      "Supervised Loss: 0.18249399345368147, Unsupervised Loss: 0.12416637688875198\n",
      "Validation Loss: 0.956484092772007, Acc: 0.759\n",
      "Supervised Loss: 0.09961970040574669, Unsupervised Loss: 0.1579686403274536\n",
      "Validation Loss: 0.9529869295656681, Acc: 0.777\n",
      "Supervised Loss: 0.0998007657751441, Unsupervised Loss: 0.1732722669839859\n",
      "Validation Loss: 1.1939248012937604, Acc: 0.752\n",
      "Supervised Loss: 0.08174725240562111, Unsupervised Loss: 0.18200775980949402\n",
      "Validation Loss: 1.0841189828235656, Acc: 0.751\n",
      "Supervised Loss: 0.060099743676837535, Unsupervised Loss: 0.19303730130195618\n",
      "Validation Loss: 1.1436228612810373, Acc: 0.763\n",
      "Supervised Loss: 0.056236920314840975, Unsupervised Loss: 0.19250008463859558\n",
      "Validation Loss: 1.0528070464357733, Acc: 0.777\n",
      "Supervised Loss: 0.05138209999771789, Unsupervised Loss: 0.19103293120861053\n",
      "Validation Loss: 1.024467221996747, Acc: 0.784\n",
      "Supervised Loss: 0.040787050516810266, Unsupervised Loss: 0.19560067355632782\n",
      "Validation Loss: 0.9963825111836195, Acc: 0.783\n",
      "Supervised Loss: 0.03580591051722877, Unsupervised Loss: 0.19719581305980682\n",
      "Validation Loss: 1.1320641902461648, Acc: 0.758\n",
      "Supervised Loss: 0.029528063110774383, Unsupervised Loss: 0.19225236773490906\n",
      "Validation Loss: 1.3706876867823303, Acc: 0.764\n",
      "Supervised Loss: 0.040745674023637546, Unsupervised Loss: 0.19378502666950226\n",
      "Validation Loss: 1.0174715119972826, Acc: 0.786\n",
      "Supervised Loss: 0.03180394686525687, Unsupervised Loss: 0.19007627665996552\n",
      "Validation Loss: 1.1079053613170982, Acc: 0.791\n",
      "Supervised Loss: 0.02556320373143535, Unsupervised Loss: 0.19059330224990845\n",
      "Validation Loss: 1.2885478706564755, Acc: 0.786\n",
      "Supervised Loss: 0.0366391023871256, Unsupervised Loss: 0.1830834299325943\n",
      "Validation Loss: 1.1135770247131587, Acc: 0.802\n",
      "Supervised Loss: 0.029954331621993333, Unsupervised Loss: 0.1850128322839737\n",
      "Validation Loss: 1.1048628773819655, Acc: 0.796\n",
      "Supervised Loss: 0.0302496049774345, Unsupervised Loss: 0.1837495118379593\n",
      "Validation Loss: 1.2701854421384633, Acc: 0.775\n",
      "Supervised Loss: 0.0325135670331656, Unsupervised Loss: 0.18294599652290344\n",
      "Validation Loss: 1.3611125690769403, Acc: 0.769\n",
      "Supervised Loss: 0.015313915581209585, Unsupervised Loss: 0.182261660695076\n",
      "Validation Loss: 1.2471166257187725, Acc: 0.786\n",
      "Supervised Loss: 0.037832027809054126, Unsupervised Loss: 0.17801780998706818\n",
      "Validation Loss: 1.1838112726807595, Acc: 0.769\n",
      "Supervised Loss: 0.03613947634235956, Unsupervised Loss: 0.18313927948474884\n",
      "Validation Loss: 1.2020853930851445, Acc: 0.78\n",
      "Supervised Loss: 0.027258120389888063, Unsupervised Loss: 0.18085478246212006\n",
      "Validation Loss: 1.184460635073483, Acc: 0.781\n",
      "Supervised Loss: 0.025543091996805743, Unsupervised Loss: 0.17506182193756104\n",
      "Validation Loss: 1.1269157502055167, Acc: 0.783\n",
      "Supervised Loss: 0.018558934528846292, Unsupervised Loss: 0.17028290033340454\n",
      "Validation Loss: 1.3994932029256597, Acc: 0.793\n",
      "Supervised Loss: 0.06199133183865342, Unsupervised Loss: 0.17851939797401428\n",
      "Validation Loss: 1.2608886560425163, Acc: 0.766\n",
      "Supervised Loss: 0.028760282409493812, Unsupervised Loss: 0.19264252483844757\n",
      "Validation Loss: 1.2228590373694896, Acc: 0.782\n",
      "Supervised Loss: 0.03317234722489957, Unsupervised Loss: 0.18767289817333221\n",
      "Validation Loss: 1.1236540711298586, Acc: 0.789\n",
      "Supervised Loss: 0.023227891392307357, Unsupervised Loss: 0.1802307665348053\n",
      "Validation Loss: 1.1339887984842063, Acc: 0.792\n",
      "Supervised Loss: 0.015709539353847505, Unsupervised Loss: 0.17420531809329987\n",
      "Validation Loss: 1.4341941076237708, Acc: 0.798\n",
      "Supervised Loss: 0.023136979068513027, Unsupervised Loss: 0.17447057366371155\n",
      "Validation Loss: 1.1502445320785046, Acc: 0.779\n",
      "Supervised Loss: 0.014514779463061131, Unsupervised Loss: 0.16332921385765076\n",
      "Validation Loss: 1.2559544948488475, Acc: 0.77\n",
      "Supervised Loss: 0.021711811442510224, Unsupervised Loss: 0.16072873771190643\n",
      "Validation Loss: 1.1082380892336368, Acc: 0.803\n",
      "Supervised Loss: 0.011292440579563846, Unsupervised Loss: 0.16191469132900238\n",
      "Validation Loss: 1.289073376553133, Acc: 0.793\n",
      "Supervised Loss: 0.01833729953126749, Unsupervised Loss: 0.16156688332557678\n",
      "Validation Loss: 1.0443175671994687, Acc: 0.806\n",
      "Supervised Loss: 0.021589572568773294, Unsupervised Loss: 0.1684051901102066\n",
      "Validation Loss: 1.250270974934101, Acc: 0.802\n",
      "Supervised Loss: 0.018262725555396174, Unsupervised Loss: 0.16447152197360992\n",
      "Validation Loss: 0.9905968520790338, Acc: 0.792\n",
      "Supervised Loss: 0.01049112212844193, Unsupervised Loss: 0.15941008925437927\n",
      "Validation Loss: 1.2300345026981085, Acc: 0.801\n",
      "Supervised Loss: 0.060905712296371345, Unsupervised Loss: 0.16802068054676056\n",
      "Validation Loss: 1.0788165068626403, Acc: 0.78\n",
      "Supervised Loss: 0.03176964901387692, Unsupervised Loss: 0.17280979454517365\n",
      "Validation Loss: 1.0918604941666126, Acc: 0.8270000000000001\n",
      "Supervised Loss: 0.029417204417986797, Unsupervised Loss: 0.1708371490240097\n",
      "Validation Loss: 1.065389221534133, Acc: 0.801\n",
      "Supervised Loss: 0.034761155156302265, Unsupervised Loss: 0.16287563741207123\n",
      "Validation Loss: 1.0500563344731928, Acc: 0.788\n",
      "Supervised Loss: 0.013860377449309453, Unsupervised Loss: 0.15459977090358734\n",
      "Validation Loss: 1.1017892146110535, Acc: 0.8160000000000001\n",
      "Supervised Loss: 0.027673886604316068, Unsupervised Loss: 0.15966497361660004\n",
      "Validation Loss: 1.1486796715669334, Acc: 0.799\n",
      "Supervised Loss: 0.012462353234004695, Unsupervised Loss: 0.1579875499010086\n",
      "Validation Loss: 1.6658661596104503, Acc: 0.763\n",
      "Supervised Loss: 0.021503778113692532, Unsupervised Loss: 0.1572043001651764\n",
      "Validation Loss: 1.07907951453235, Acc: 0.81\n",
      "Supervised Loss: 0.033528994020307436, Unsupervised Loss: 0.15751425921916962\n",
      "Validation Loss: 1.2378357576020063, Acc: 0.802\n",
      "Supervised Loss: 0.012196124283655081, Unsupervised Loss: 0.1564408838748932\n",
      "Validation Loss: 1.266773462444544, Acc: 0.797\n",
      "Supervised Loss: 0.024796189722255804, Unsupervised Loss: 0.1590103954076767\n",
      "Validation Loss: 1.1668341733515262, Acc: 0.811\n",
      "Supervised Loss: 0.01824567420233507, Unsupervised Loss: 0.1618468463420868\n",
      "Validation Loss: 1.3227372433245181, Acc: 0.797\n",
      "Supervised Loss: 0.01766386850416893, Unsupervised Loss: 0.15340068936347961\n",
      "Validation Loss: 1.394283906891942, Acc: 0.775\n",
      "Supervised Loss: 0.020595656374352983, Unsupervised Loss: 0.15128740668296814\n",
      "Validation Loss: 1.0775413224101067, Acc: 0.8200000000000001\n",
      "Supervised Loss: 0.011789328273735009, Unsupervised Loss: 0.15177342295646667\n",
      "Validation Loss: 1.2389803215116262, Acc: 0.8150000000000001\n",
      "Supervised Loss: 0.017063156163785606, Unsupervised Loss: 0.15556132793426514\n",
      "Validation Loss: 1.2173816721141337, Acc: 0.8230000000000001\n",
      "Supervised Loss: 0.01798177334887441, Unsupervised Loss: 0.1549907624721527\n",
      "Validation Loss: 0.9953773464262485, Acc: 0.8160000000000001\n",
      "Supervised Loss: 0.01520589972496964, Unsupervised Loss: 0.1586795300245285\n",
      "Validation Loss: 1.2851776270288975, Acc: 0.812\n",
      "Supervised Loss: 0.013467559748678469, Unsupervised Loss: 0.1618446558713913\n",
      "Validation Loss: 1.6491260290518404, Acc: 0.777\n",
      "Supervised Loss: 0.03536640986421844, Unsupervised Loss: 0.15536336600780487\n",
      "Validation Loss: 1.2792680739611386, Acc: 0.796\n",
      "Supervised Loss: 0.009275611543562262, Unsupervised Loss: 0.14737115800380707\n",
      "Validation Loss: 1.1504970591235906, Acc: 0.8200000000000001\n",
      "Supervised Loss: 0.036129782382631676, Unsupervised Loss: 0.15285387635231018\n",
      "Validation Loss: 1.0830734602594747, Acc: 0.8\n",
      "Supervised Loss: 0.015007166029827204, Unsupervised Loss: 0.1505664736032486\n",
      "Validation Loss: 1.254999870257452, Acc: 0.798\n",
      "Supervised Loss: 0.02461608724639518, Unsupervised Loss: 0.15415321290493011\n",
      "Validation Loss: 1.06239553142339, Acc: 0.8170000000000001\n",
      "Supervised Loss: 0.012732239309407305, Unsupervised Loss: 0.1529131680727005\n",
      "Validation Loss: 1.2226976220309735, Acc: 0.808\n",
      "Supervised Loss: 0.010572601810563355, Unsupervised Loss: 0.15410444140434265\n",
      "Validation Loss: 1.2257571551948785, Acc: 0.8220000000000001\n",
      "Supervised Loss: 0.02517746804835042, Unsupervised Loss: 0.155310720205307\n",
      "Validation Loss: 1.1444649438560008, Acc: 0.836\n",
      "Supervised Loss: 0.01631581730529433, Unsupervised Loss: 0.14942263066768646\n",
      "Validation Loss: 1.1756798696517945, Acc: 0.8160000000000001\n",
      "Supervised Loss: 0.01642322938569123, Unsupervised Loss: 0.15139886736869812\n",
      "Validation Loss: 1.3005249578878284, Acc: 0.812\n",
      "Supervised Loss: 0.01601619128428865, Unsupervised Loss: 0.1493842452764511\n",
      "Validation Loss: 1.249749566912651, Acc: 0.809\n",
      "Supervised Loss: 0.009734708078904078, Unsupervised Loss: 0.1503984034061432\n",
      "Validation Loss: 1.388007542481646, Acc: 0.8180000000000001\n",
      "Supervised Loss: 0.013626591710199136, Unsupervised Loss: 0.1515669971704483\n",
      "Validation Loss: 1.184302767701447, Acc: 0.8240000000000001\n",
      "Supervised Loss: 0.022817899042274804, Unsupervised Loss: 0.15710748732089996\n",
      "Validation Loss: 1.1984252209961415, Acc: 0.812\n",
      "Supervised Loss: 0.015524183745728806, Unsupervised Loss: 0.1435091346502304\n",
      "Validation Loss: 1.3951160204503685, Acc: 0.8170000000000001\n",
      "Supervised Loss: 0.01634303168277256, Unsupervised Loss: 0.1486910879611969\n",
      "Validation Loss: 1.2918211335968226, Acc: 0.8150000000000001\n",
      "Supervised Loss: 0.011197643229097594, Unsupervised Loss: 0.14719510078430176\n",
      "Validation Loss: 1.3372742664441466, Acc: 0.8210000000000001\n",
      "Supervised Loss: 0.022275560740818035, Unsupervised Loss: 0.15107902884483337\n",
      "Validation Loss: 1.0985189129412174, Acc: 0.8310000000000001\n",
      "Supervised Loss: 0.014214891543670092, Unsupervised Loss: 0.14720652997493744\n",
      "Validation Loss: 1.0320776607841253, Acc: 0.8250000000000001\n",
      "Supervised Loss: 0.016399081908457448, Unsupervised Loss: 0.14520263671875\n",
      "Validation Loss: 1.1785681094229221, Acc: 0.8180000000000001\n",
      "Supervised Loss: 0.01576206279307371, Unsupervised Loss: 0.14680348336696625\n",
      "Validation Loss: 1.2299340583756566, Acc: 0.811\n",
      "Supervised Loss: 0.02394139884854667, Unsupervised Loss: 0.1495729237794876\n",
      "Validation Loss: 1.1178909412398934, Acc: 0.8270000000000001\n",
      "Supervised Loss: 0.0263107864340418, Unsupervised Loss: 0.14764469861984253\n",
      "Validation Loss: 1.0812090101838112, Acc: 0.8230000000000001\n",
      "Supervised Loss: 0.013854545010108268, Unsupervised Loss: 0.15204894542694092\n",
      "Validation Loss: 1.1225034615397453, Acc: 0.8260000000000001\n",
      "Supervised Loss: 0.019534180657792603, Unsupervised Loss: 0.14799338579177856\n",
      "Validation Loss: 1.1948138676676898, Acc: 0.8220000000000001\n",
      "Supervised Loss: 0.008342659571790137, Unsupervised Loss: 0.1430279165506363\n",
      "Validation Loss: 1.1924679135158658, Acc: 0.8150000000000001\n",
      "Supervised Loss: 0.011725196495535783, Unsupervised Loss: 0.14405891299247742\n",
      "Validation Loss: 0.9195019546290859, Acc: 0.8280000000000001\n",
      "Supervised Loss: 0.013696340834139847, Unsupervised Loss: 0.13999952375888824\n",
      "Validation Loss: 1.2767021412588655, Acc: 0.8130000000000001\n",
      "Supervised Loss: 0.0731528527289629, Unsupervised Loss: 0.16446362435817719\n",
      "Validation Loss: 1.422183968424797, Acc: 0.781\n",
      "Supervised Loss: 0.010732535337738228, Unsupervised Loss: 0.14664866030216217\n"
     ]
    }
   ],
   "source": [
    "train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, num_epochs, eval_step, threshold, lambda_u=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "498ccd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a61e15ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 75, 75]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 75, 75]             128\n",
      "              ReLU-3           [-1, 64, 75, 75]               0\n",
      "         MaxPool2d-4           [-1, 64, 38, 38]               0\n",
      "            Conv2d-5           [-1, 64, 38, 38]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 38, 38]             128\n",
      "              ReLU-7           [-1, 64, 38, 38]               0\n",
      "            Conv2d-8           [-1, 64, 38, 38]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 38, 38]             128\n",
      "             ReLU-10           [-1, 64, 38, 38]               0\n",
      "       BasicBlock-11           [-1, 64, 38, 38]               0\n",
      "           Conv2d-12           [-1, 64, 38, 38]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 38, 38]             128\n",
      "             ReLU-14           [-1, 64, 38, 38]               0\n",
      "           Conv2d-15           [-1, 64, 38, 38]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 38, 38]             128\n",
      "             ReLU-17           [-1, 64, 38, 38]               0\n",
      "       BasicBlock-18           [-1, 64, 38, 38]               0\n",
      "           Conv2d-19          [-1, 128, 19, 19]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 19, 19]             256\n",
      "             ReLU-21          [-1, 128, 19, 19]               0\n",
      "           Conv2d-22          [-1, 128, 19, 19]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 19, 19]             256\n",
      "           Conv2d-24          [-1, 128, 19, 19]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 19, 19]             256\n",
      "             ReLU-26          [-1, 128, 19, 19]               0\n",
      "       BasicBlock-27          [-1, 128, 19, 19]               0\n",
      "           Conv2d-28          [-1, 128, 19, 19]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 19, 19]             256\n",
      "             ReLU-30          [-1, 128, 19, 19]               0\n",
      "           Conv2d-31          [-1, 128, 19, 19]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 19, 19]             256\n",
      "             ReLU-33          [-1, 128, 19, 19]               0\n",
      "       BasicBlock-34          [-1, 128, 19, 19]               0\n",
      "           Conv2d-35          [-1, 256, 10, 10]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 10, 10]             512\n",
      "             ReLU-37          [-1, 256, 10, 10]               0\n",
      "           Conv2d-38          [-1, 256, 10, 10]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 10, 10]             512\n",
      "           Conv2d-40          [-1, 256, 10, 10]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 10, 10]             512\n",
      "             ReLU-42          [-1, 256, 10, 10]               0\n",
      "       BasicBlock-43          [-1, 256, 10, 10]               0\n",
      "           Conv2d-44          [-1, 256, 10, 10]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 10, 10]             512\n",
      "             ReLU-46          [-1, 256, 10, 10]               0\n",
      "           Conv2d-47          [-1, 256, 10, 10]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 10, 10]             512\n",
      "             ReLU-49          [-1, 256, 10, 10]               0\n",
      "       BasicBlock-50          [-1, 256, 10, 10]               0\n",
      "           Conv2d-51            [-1, 512, 5, 5]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-53            [-1, 512, 5, 5]               0\n",
      "           Conv2d-54            [-1, 512, 5, 5]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 5, 5]           1,024\n",
      "           Conv2d-56            [-1, 512, 5, 5]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-58            [-1, 512, 5, 5]               0\n",
      "       BasicBlock-59            [-1, 512, 5, 5]               0\n",
      "           Conv2d-60            [-1, 512, 5, 5]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-62            [-1, 512, 5, 5]               0\n",
      "           Conv2d-63            [-1, 512, 5, 5]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-65            [-1, 512, 5, 5]               0\n",
      "       BasicBlock-66            [-1, 512, 5, 5]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                    [-1, 8]           4,104\n",
      "           ResNet-69                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 11,180,616\n",
      "Trainable params: 11,180,616\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 29.15\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 72.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 150, 150))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
